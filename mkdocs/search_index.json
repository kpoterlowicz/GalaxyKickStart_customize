{
    "docs": [
        {
            "location": "/", 
            "text": "GalaxyKickStart\n\n\nGalaxyKickStart is an \nAnsible\n playbook designed for installing, testing, deploying and \nmaintaining production-grade Galaxy instances.\n\nIn the basic configuration, this includes:\n\n\n\n\npostgresql server as database backend \n\n\nnginx proxy \n\n\nslurm cluster\n\n\n\n\nIn adition, tools and workflows can be managed.", 
            "title": "Home"
        }, 
        {
            "location": "/#galaxykickstart", 
            "text": "GalaxyKickStart is an  Ansible  playbook designed for installing, testing, deploying and \nmaintaining production-grade Galaxy instances. \nIn the basic configuration, this includes:   postgresql server as database backend   nginx proxy   slurm cluster   In adition, tools and workflows can be managed.", 
            "title": "GalaxyKickStart"
        }, 
        {
            "location": "/about/", 
            "text": "GalaxyKickStart\n\n\nGalaxyKickStart is an Ansible playbook designed to help you get one or more production-ready\n \nGalaxy servers\n based on Ubuntu within minutes, and to maintain these servers.\n\n\nRequired ansible version \n= 2.1.2.0\n\n\nOptionally, instances can be pre-loaded with tools and workflows.\n\n\nThe playbook has been tested on \n\n\n\n\nCloud Machines\n\n\nVagrant Boxes\n\n\nPhysical Servers \n\n\nDocker.\n\n\n\n\nGalaxyKickStart has been developed at the \nARTbio platform\n and contains roles developed\nby the \nGalaxy team\n.\n\n\nList of roles included in this playbook\n\n\nansible-postgresql-objects role\n\n\nensure_postgresql_up role\n\n\ngalaxy-extras role\n\n\ngalaxy-tools role\n\n\ngalaxy-os role\n\n\ngalaxy role\n\n\nansible-trackster role\n\n\nminiconda role", 
            "title": "What is GalaxyKickStart"
        }, 
        {
            "location": "/about/#galaxykickstart", 
            "text": "GalaxyKickStart is an Ansible playbook designed to help you get one or more production-ready\n  Galaxy servers  based on Ubuntu within minutes, and to maintain these servers.", 
            "title": "GalaxyKickStart"
        }, 
        {
            "location": "/about/#required-ansible-version-2120", 
            "text": "Optionally, instances can be pre-loaded with tools and workflows.  The playbook has been tested on    Cloud Machines  Vagrant Boxes  Physical Servers   Docker.   GalaxyKickStart has been developed at the  ARTbio platform  and contains roles developed\nby the  Galaxy team .", 
            "title": "Required ansible version &gt;= 2.1.2.0"
        }, 
        {
            "location": "/about/#list-of-roles-included-in-this-playbook", 
            "text": "ansible-postgresql-objects role  ensure_postgresql_up role  galaxy-extras role  galaxy-tools role  galaxy-os role  galaxy role  ansible-trackster role  miniconda role", 
            "title": "List of roles included in this playbook"
        }, 
        {
            "location": "/getting_started/", 
            "text": "Getting Started\n\n\nYou need \ngit\n installed\n\n\nMake sure that you have a recent version of \nAnsible\n installed\n\n\nThe playbook has been tested with Ansible stable versions 2.1 and 2.2\n\n\nInstall Ansible with pip\n\n\nA simple way to install the latest Ansible version is using \npip\n:\n\n\n\n\nEnsure you have recent pip version installed (sudo -i \n pip install upgrade pip maybe necessary)\n\n\n\n\n$ pip --version\npip 9.0.1 from /usr/local/lib/python2.7/site-packages (python 2.7)\n\n\n\n\n\n\nThen\n\n\n\n\npip install ansible==2.2.0.0\n\n\n\n\nInstall Ansible with apt\n\n\nAlternatively, Ansible may be installed with the Apt package manager (Ubuntu):\n\n\nsudo -i\napt-get install software-properties-common\napt-add-repository ppa:ansible/ansible\napt-get update\napt-get install ansible\n\n\n\n\nIn some occasions, additional packages may be necessary for correct Ansible installation:\n\n\napt-get update \n apt-get -y install build-essential libpq-dev python-dev libxml2-dev libxslt1-dev libldap2-dev libsasl2-dev libffi-dev\n\n\n\n\nGetting the playbook\n\n\nGalaxyKickStart is hosted on\n\ngithub\n and uses a number of\ndependent Ansible roles that need to be downloaded as part of the installation\nstep:\n\n\ngit clone https://github.com/ARTbio/GalaxyKickStart.git\ncd GalaxyKickStart\nansible-galaxy install -r requirements_roles.yml -p roles\n\n\n\n\nThe playbook (here \ngalaxy.yml\n) should be in the GalaxyKickStart folder.\n\n\nls\nCONTRIBUTORS.md     Vagrantfile     docs            inventory_files     roles\nDockerfile      ansible.cfg     extra-files     mkdocs.yml      scripts\nLICENSE.txt     deploy.sh       galaxy.yml      pre-commit.sh       startup.sh\nREADME.md       dockerfiles     group_vars      requirements_roles.yml  templates\n\n\n\n\nDeploying galaxy-kickstart on remote machines.\n\n\n\n\nInside the \ninventory_files\n folder, you will find a number of inventory files.\nThis is an example of inventory taken from the \nartimed\n inventory file.\n\n\n[artimed]\nlocalhost ansible_ssh_user=\nroot\n ansible_ssh_private_key_file=\n~/.ssh/id_rsa\n\n...\n\n\n\n\nHere \n[artimed]\n is a group, that contains a machine called localhost.\nThe variables defined in \ngroup_vars/artimed\n will be applied to this host.\nAnsible will connect by ssh to this machine, using the ssh key in \n~/.ssh/id_rsa\n.\n\n\nIf you would like to run this playbook on a remote machine by ssh (currently needs to be a debian-type machine),\ncreate a new inventory, and change \nlocalhost\n to the IP address of that machine.\n\nansible_ssh_user=\nuser\n controls under which username to connect to this machine.\nThis user needs to have sudo rights.\n\n\nThen, run the plabook by typing:\n\n\nansible-playbook --inventory-file inventory_files/\nyour_inventory_file\n galaxy.yml\n\n\n\n\nYou can put multiple machines in your inventory.\nIf you run the playbook a second time, the process will be much faster, since steps that have already been executed will be skipped.\nWhenever you change a variable (see \ncustomizations\n), you need to run the playbook again.\n\n\nDeploying galaxy-kickstart on specified clouds\n\n\nInside the repository you will find a \nfile\n\ncalled \ninventory_files/cloud\n. This file serves as an example hosts file for\nhow to deploy galaxy-kickstart on Google Compute Engine (GCE),  Amazon Web\nServices(aws), and Jetstream (OpenStack). \nPlease note that the \nansible_user\n\nvariable in the file changes for each remote target\n. If you are wanting to use\nthis playbook on a cloud other than the ones listed  below, you will need to\nupdate the inventory to add a new section header for the respective target. If\nthis happens to be a cloud setup, make sure to add the section header under\n\n[cloud_setup:children]\n.\n\n\nSpecifications for each remote target:\n\n\n\n\n\n\nOpenStack\n\n\n\n\nImage needed to deploy on \nJetstream\n:\n    \nUbuntu 14.04.3 Development (jetstream image id: d7fe3289-943f-4417-90e8-8a6b171677ca)\n\n\nInventory: \nremote host IP\n anisble_ssh_user=\"root\" ansible_ssh_private_key_file=\"\npath/to/your/private/key\n\"\n\n\n\n\n\n\n\n\nGCE\n\n\n\n\nImage needed to deploy galaxy-kickstart: \nUbuntu 14.04 LTS\n\n\nInventory: \nremote host IP\n anisble_ssh_user=\"ubuntu\" ansible_ssh_private_key_file=\"\npath/to/your/private/key\n\"\n\n\n\n\n\n\n\n\nAWS\n\n\n\n\nImage needed to deploy galaxy-kickstart: \nUbuntu Server 14.04 LTS (HVM), SSD Volume Type - ami-2d39803a\n\n\nInventory: \ntarget Amazon Web Services IP address\n ansible_ssh_user=\"ubuntu\" ansible_ssh_private_key_file=\"\npath/to/your/aws/private/key\n\"", 
            "title": "Getting started"
        }, 
        {
            "location": "/getting_started/#getting-started", 
            "text": "", 
            "title": "Getting Started"
        }, 
        {
            "location": "/getting_started/#you-need-git-installed", 
            "text": "", 
            "title": "You need git installed"
        }, 
        {
            "location": "/getting_started/#make-sure-that-you-have-a-recent-version-of-ansible-installed", 
            "text": "The playbook has been tested with Ansible stable versions 2.1 and 2.2", 
            "title": "Make sure that you have a recent version of Ansible installed"
        }, 
        {
            "location": "/getting_started/#install-ansible-with-pip", 
            "text": "A simple way to install the latest Ansible version is using  pip :   Ensure you have recent pip version installed (sudo -i   pip install upgrade pip maybe necessary)   $ pip --version\npip 9.0.1 from /usr/local/lib/python2.7/site-packages (python 2.7)   Then   pip install ansible==2.2.0.0", 
            "title": "Install Ansible with pip"
        }, 
        {
            "location": "/getting_started/#install-ansible-with-apt", 
            "text": "Alternatively, Ansible may be installed with the Apt package manager (Ubuntu):  sudo -i\napt-get install software-properties-common\napt-add-repository ppa:ansible/ansible\napt-get update\napt-get install ansible", 
            "title": "Install Ansible with apt"
        }, 
        {
            "location": "/getting_started/#in-some-occasions-additional-packages-may-be-necessary-for-correct-ansible-installation", 
            "text": "apt-get update   apt-get -y install build-essential libpq-dev python-dev libxml2-dev libxslt1-dev libldap2-dev libsasl2-dev libffi-dev", 
            "title": "In some occasions, additional packages may be necessary for correct Ansible installation:"
        }, 
        {
            "location": "/getting_started/#getting-the-playbook", 
            "text": "GalaxyKickStart is hosted on github  and uses a number of\ndependent Ansible roles that need to be downloaded as part of the installation\nstep:  git clone https://github.com/ARTbio/GalaxyKickStart.git\ncd GalaxyKickStart\nansible-galaxy install -r requirements_roles.yml -p roles  The playbook (here  galaxy.yml ) should be in the GalaxyKickStart folder.  ls\nCONTRIBUTORS.md     Vagrantfile     docs            inventory_files     roles\nDockerfile      ansible.cfg     extra-files     mkdocs.yml      scripts\nLICENSE.txt     deploy.sh       galaxy.yml      pre-commit.sh       startup.sh\nREADME.md       dockerfiles     group_vars      requirements_roles.yml  templates", 
            "title": "Getting the playbook"
        }, 
        {
            "location": "/getting_started/#deploying-galaxy-kickstart-on-remote-machines", 
            "text": "Inside the  inventory_files  folder, you will find a number of inventory files.\nThis is an example of inventory taken from the  artimed  inventory file.  [artimed]\nlocalhost ansible_ssh_user= root  ansible_ssh_private_key_file= ~/.ssh/id_rsa \n...  Here  [artimed]  is a group, that contains a machine called localhost.\nThe variables defined in  group_vars/artimed  will be applied to this host.\nAnsible will connect by ssh to this machine, using the ssh key in  ~/.ssh/id_rsa .  If you would like to run this playbook on a remote machine by ssh (currently needs to be a debian-type machine),\ncreate a new inventory, and change  localhost  to the IP address of that machine. ansible_ssh_user= user  controls under which username to connect to this machine.\nThis user needs to have sudo rights.  Then, run the plabook by typing:  ansible-playbook --inventory-file inventory_files/ your_inventory_file  galaxy.yml  You can put multiple machines in your inventory.\nIf you run the playbook a second time, the process will be much faster, since steps that have already been executed will be skipped.\nWhenever you change a variable (see  customizations ), you need to run the playbook again.", 
            "title": "Deploying galaxy-kickstart on remote machines."
        }, 
        {
            "location": "/getting_started/#deploying-galaxy-kickstart-on-specified-clouds", 
            "text": "Inside the repository you will find a  file \ncalled  inventory_files/cloud . This file serves as an example hosts file for\nhow to deploy galaxy-kickstart on Google Compute Engine (GCE),  Amazon Web\nServices(aws), and Jetstream (OpenStack).  Please note that the  ansible_user \nvariable in the file changes for each remote target . If you are wanting to use\nthis playbook on a cloud other than the ones listed  below, you will need to\nupdate the inventory to add a new section header for the respective target. If\nthis happens to be a cloud setup, make sure to add the section header under [cloud_setup:children] .  Specifications for each remote target:    OpenStack   Image needed to deploy on  Jetstream :\n     Ubuntu 14.04.3 Development (jetstream image id: d7fe3289-943f-4417-90e8-8a6b171677ca)  Inventory:  remote host IP  anisble_ssh_user=\"root\" ansible_ssh_private_key_file=\" path/to/your/private/key \"     GCE   Image needed to deploy galaxy-kickstart:  Ubuntu 14.04 LTS  Inventory:  remote host IP  anisble_ssh_user=\"ubuntu\" ansible_ssh_private_key_file=\" path/to/your/private/key \"     AWS   Image needed to deploy galaxy-kickstart:  Ubuntu Server 14.04 LTS (HVM), SSD Volume Type - ami-2d39803a  Inventory:  target Amazon Web Services IP address  ansible_ssh_user=\"ubuntu\" ansible_ssh_private_key_file=\" path/to/your/aws/private/key \"", 
            "title": "Deploying galaxy-kickstart on specified clouds"
        }, 
        {
            "location": "/customizations/", 
            "text": "Customising the playbook\n\n\nWe strongly encourage users to read the \nansible inventory\n documentation first.\n\n\nMost settings should be editable without modifying the playbook directly,\ninstead variables can be set in group_vars and host vars.\n\n\nThe playbook comes with an example inventory file \nhosts\n.\n\n\n[artimed]\nlocalhost ansible_ssh_user=\nroot\n ansible_ssh_private_key_file=\n~/.ssh/id_rsa\n\n[travis_bioblend]\nlocalhost ansible_connection=local\n[aws]\n# Put you aws IP and key here to make FTP work in the default VPC.\n# If you want further group-specific variables, put the host in these groups as well [e.g artimed].\n\n\n\n\n[artimed]\n, \n[travis_bioblend]\n and \n[aws]\n are predefined groups. Any host (here we only have localhost) that\nis added to one or multiple groups will have the corresponding group variables applied.\nGroup variables are defined in \ngroup_vars/[name of the group]\n and default variables are found in \n\n\ngroup_vars/all\n.\nAll variables defined in \ngroup_vars/all\n are overwritten in \ngroup_vars/[name of the group]\n.  \n\n\nFor instance the variable \nproftpd_nat_masquerade\n is set to \nfalse\n in \ngroup_vars/all\n, while hosts in the \n[aws]\n group\napply the \n[aws]\n group variables which set \nproftpd_nat_masquerade\n to true, so that hosts in the aws group will have\nthis aws-specific setting applied. Any combination of groups may be used.\n\n\nIf you want to apply any of the changes you made to the variables you need to run the playbook again, making sure that\nthe host you are targeting is in the right group. The simplest way to do so is to use an inventory file that only contains\nthe group and the host you wish to target. If this is for example the group metavisitor, and you target the host localhost,\nyour inventory file should look like this:\n\n\n[metavisitor]\nlocalhost\n\n\n\n\nYou can then run the playbook as usual:\n\n\nansible-playbook --inventory-file=\nyour_inventory_file\n galaxy.yml\n\n\n\n\nImportant variables\n\n\nWe aimed for this playbook to be reusable. We therefore made most variables configurable.\nThe group_vars/all file contains the variables we have chosen as defaults. You may override them either in this file\nor you can use ansible group variables to selectively set the variables for certain hosts/groups. See the \nansible documentation\nabout group variables\n for details.\n\n\nThese most important variables are:\n\n\n\n\n\n\nansible_ssh_user - The login name used to access the target.\n\n\n\n\n\n\nansible_ssh_private_key_file - The ssh private key used to access the target.\n\n\n\n\n\n\ninstall_galaxy - True for install a Galaxy instance.\n\n\n\n\n\n\ninstall_tools - True for install the NGS tools.\n\n\n\n\n\n\nrun_data_manager - True for run the data manager procedure.\n\n\n\n\n\n\ngalaxy_user_name - The Operating System user name for galaxy process.\n\n\n\n\n\n\ngalaxy_server_dir - The home of Operating System user for galaxy process.\n\n\n\n\n\n\ngalaxy_admin - The admin galaxy user.\n\n\n\n\n\n\ngalaxy_admin_pw - The admin galaxy password.\n\n\n\n\n\n\ndefault_admin_api_key - The api key for tool installation and download reference genomes throught galaxy data managers. To be removed in production.\n\n\n\n\n\n\ngalaxy_tool_list - The files that constants the list of tools to be installed.\n\n\n\n\n\n\ngalaxy_data_managers - The reference genomes and indexes to be load and build.\n\n\n\n\n\n\ngalaxy_data - The persistent directory where the galaxy config and database directories will be installed or will be recovered.\n\n\n\n\n\n\ngalaxy_database - The persistent directory where postgresql will be installed or will be recovered.\n\n\n\n\n\n\ngalaxy_db - Connection string for galaxy-postgresql.\n\n\n\n\n\n\ngalaxy_changeset_id - The release of Galaxy to be installed (master, dev or release_xx_xx).", 
            "title": "Customizations"
        }, 
        {
            "location": "/customizations/#customising-the-playbook", 
            "text": "We strongly encourage users to read the  ansible inventory  documentation first.  Most settings should be editable without modifying the playbook directly,\ninstead variables can be set in group_vars and host vars.  The playbook comes with an example inventory file  hosts .  [artimed]\nlocalhost ansible_ssh_user= root  ansible_ssh_private_key_file= ~/.ssh/id_rsa \n[travis_bioblend]\nlocalhost ansible_connection=local\n[aws]\n# Put you aws IP and key here to make FTP work in the default VPC.\n# If you want further group-specific variables, put the host in these groups as well [e.g artimed].  [artimed] ,  [travis_bioblend]  and  [aws]  are predefined groups. Any host (here we only have localhost) that\nis added to one or multiple groups will have the corresponding group variables applied.\nGroup variables are defined in  group_vars/[name of the group]  and default variables are found in   group_vars/all .\nAll variables defined in  group_vars/all  are overwritten in  group_vars/[name of the group] .    For instance the variable  proftpd_nat_masquerade  is set to  false  in  group_vars/all , while hosts in the  [aws]  group\napply the  [aws]  group variables which set  proftpd_nat_masquerade  to true, so that hosts in the aws group will have\nthis aws-specific setting applied. Any combination of groups may be used.  If you want to apply any of the changes you made to the variables you need to run the playbook again, making sure that\nthe host you are targeting is in the right group. The simplest way to do so is to use an inventory file that only contains\nthe group and the host you wish to target. If this is for example the group metavisitor, and you target the host localhost,\nyour inventory file should look like this:  [metavisitor]\nlocalhost  You can then run the playbook as usual:  ansible-playbook --inventory-file= your_inventory_file  galaxy.yml", 
            "title": "Customising the playbook"
        }, 
        {
            "location": "/customizations/#important-variables", 
            "text": "We aimed for this playbook to be reusable. We therefore made most variables configurable.\nThe group_vars/all file contains the variables we have chosen as defaults. You may override them either in this file\nor you can use ansible group variables to selectively set the variables for certain hosts/groups. See the  ansible documentation\nabout group variables  for details.  These most important variables are:    ansible_ssh_user - The login name used to access the target.    ansible_ssh_private_key_file - The ssh private key used to access the target.    install_galaxy - True for install a Galaxy instance.    install_tools - True for install the NGS tools.    run_data_manager - True for run the data manager procedure.    galaxy_user_name - The Operating System user name for galaxy process.    galaxy_server_dir - The home of Operating System user for galaxy process.    galaxy_admin - The admin galaxy user.    galaxy_admin_pw - The admin galaxy password.    default_admin_api_key - The api key for tool installation and download reference genomes throught galaxy data managers. To be removed in production.    galaxy_tool_list - The files that constants the list of tools to be installed.    galaxy_data_managers - The reference genomes and indexes to be load and build.    galaxy_data - The persistent directory where the galaxy config and database directories will be installed or will be recovered.    galaxy_database - The persistent directory where postgresql will be installed or will be recovered.    galaxy_db - Connection string for galaxy-postgresql.    galaxy_changeset_id - The release of Galaxy to be installed (master, dev or release_xx_xx).", 
            "title": "Important variables"
        }, 
        {
            "location": "/installing tools and workflows/", 
            "text": "Installing tools\n\n\n\n\nThis playbook includes the \nansible-galaxy-tools\n role which can be used\nto install tools and workflows into galaxy instances using the \nbioblend\n api.  \n\n\nCreating a tool_list.yml file\n\n\nTo install tools, you will need to prepare a list of tools in yaml format.\nA an example of a a tool list can be found in \nhere\n\n\ntools:\n- name: blast_to_scaffold\n  owner: drosofff\n  revisions:\n  tool_panel_section_label: Metavisitor\n  tool_shed_url: https://toolshed.g2.bx.psu.edu/\n- name: blastx_to_scaffold\n  owner: drosofff\n  revisions:\n  tool_panel_section_label: Metavisitor\n  tool_shed_url: https://toolshed.g2.bx.psu.edu/\n- name: bowtie2\n  owner: devteam\n  revisions:\n  - 019c2a81547a\n  tool_panel_section_label: Metavisitor\n  tool_shed_url: https://toolshed.g2.bx.psu.edu/\n\n\n\n\nwhen the revision is empty, the latest available revision will be installed.\n\ntool_panel_section_label will determine the tool panel section where the tools will be found.\n\n\nObtaining a tool_list.yml file\n\n\nWe can also obtain a tool list from a runnning galaxy instance.\nNote that for server running a galaxy release \n16.04, you need a galaxy API keys and bioblend.\nA script is included in the extra-files directory.\n\n\npython get_tool_yml_from_gi.py --galaxy \nmy_galaxy_url\n --api-key \nmy_admin_api_key\n --output-file \nmy_tool_list.yml\n\n\n\n\n\nAdding a tool_list.yml file to a group_variable files\n\n\nGroup variable files are in the group_vars directory.\n\n\nIf you would like to install tools, you need to reference the tool_list.yml in the group variable file.\nWe typically place additional files in the \nextra-files/\nhostname\n/\nhostname\n_tool_list.yml\n file.\n\n\nIf you would like to add tools to a group that is called metavisitor edit \ngroup_vars/metavisitor\n and add these lines:\n\n\ninstall_tools: true\ngalaxy_tools_tool_list: \nextra-files/metavisitor/metavisitor_tool_list.yml\n\n\n\n\n\nInstalling workflows\n\n\nYou can also make sure that workflows are available after running the playbook.\nAs with tools, place the workflows in \nextra-files/\nhostname\n/\nhostname\nworkflow_name\n.ga\n\nAdd these lines to the corresponding group_var file:\n\n\ngalaxy_tools_install_workflows: true\ngalaxy_tools_workflows:\n  - \nextra-files/metavisitor/Galaxy-Workflow-create_model.ga\n\n  - \nextra-files/metavisitor/Galaxy-Workflow-separate_host_and_virus_reads.ga\n\n  - \nextra-files/metavisitor/Galaxy-Workflow-standart_metavisitor_workflow_(input__clipped_dataset).ga\n\n  - \nextra-files/metavisitor/Galaxy-Workflow-Metavisitor_Test_case_1-1_Guided.ga\n\n  - \nextra-files/metavisitor/Galaxy-Workflow-Metavisitor_Test_case_1-2_Guided.ga\n\n  - \nextra-files/metavisitor/Galaxy-Workflow-Metavisitor_Test_case_1-3_Guided.ga\n\n  - \nextra-files/metavisitor/Galaxy-Workflow-Meta-visitor__test_case_Nora_virus,_REMAPPING.ga\n\n\n\n\n\nRunning the playbook\n\n\nAs per usual, run the playbook with an inventory file that maps your target machine to the metavisitor group.\nIf the target is localhost, your inventory file should look ike this:\n\n\n[metavisitor]\nlocalhost\n\n\n\n\nthen run the playbook like so:\n\n\nansible-playbook --inventory-file=\nyour_inventory_file\n galaxy.yml", 
            "title": "Installing tools and workflows"
        }, 
        {
            "location": "/installing tools and workflows/#installing-tools", 
            "text": "This playbook includes the  ansible-galaxy-tools  role which can be used\nto install tools and workflows into galaxy instances using the  bioblend  api.", 
            "title": "Installing tools"
        }, 
        {
            "location": "/installing tools and workflows/#creating-a-tool_listyml-file", 
            "text": "To install tools, you will need to prepare a list of tools in yaml format.\nA an example of a a tool list can be found in  here  tools:\n- name: blast_to_scaffold\n  owner: drosofff\n  revisions:\n  tool_panel_section_label: Metavisitor\n  tool_shed_url: https://toolshed.g2.bx.psu.edu/\n- name: blastx_to_scaffold\n  owner: drosofff\n  revisions:\n  tool_panel_section_label: Metavisitor\n  tool_shed_url: https://toolshed.g2.bx.psu.edu/\n- name: bowtie2\n  owner: devteam\n  revisions:\n  - 019c2a81547a\n  tool_panel_section_label: Metavisitor\n  tool_shed_url: https://toolshed.g2.bx.psu.edu/  when the revision is empty, the latest available revision will be installed. \ntool_panel_section_label will determine the tool panel section where the tools will be found.", 
            "title": "Creating a tool_list.yml file"
        }, 
        {
            "location": "/installing tools and workflows/#obtaining-a-tool_listyml-file", 
            "text": "We can also obtain a tool list from a runnning galaxy instance.\nNote that for server running a galaxy release  16.04, you need a galaxy API keys and bioblend.\nA script is included in the extra-files directory.  python get_tool_yml_from_gi.py --galaxy  my_galaxy_url  --api-key  my_admin_api_key  --output-file  my_tool_list.yml", 
            "title": "Obtaining a tool_list.yml file"
        }, 
        {
            "location": "/installing tools and workflows/#adding-a-tool_listyml-file-to-a-group_variable-files", 
            "text": "Group variable files are in the group_vars directory.  If you would like to install tools, you need to reference the tool_list.yml in the group variable file.\nWe typically place additional files in the  extra-files/ hostname / hostname _tool_list.yml  file.  If you would like to add tools to a group that is called metavisitor edit  group_vars/metavisitor  and add these lines:  install_tools: true\ngalaxy_tools_tool_list:  extra-files/metavisitor/metavisitor_tool_list.yml", 
            "title": "Adding a tool_list.yml file to a group_variable files"
        }, 
        {
            "location": "/installing tools and workflows/#installing-workflows", 
            "text": "You can also make sure that workflows are available after running the playbook.\nAs with tools, place the workflows in  extra-files/ hostname / hostname workflow_name .ga \nAdd these lines to the corresponding group_var file:  galaxy_tools_install_workflows: true\ngalaxy_tools_workflows:\n  -  extra-files/metavisitor/Galaxy-Workflow-create_model.ga \n  -  extra-files/metavisitor/Galaxy-Workflow-separate_host_and_virus_reads.ga \n  -  extra-files/metavisitor/Galaxy-Workflow-standart_metavisitor_workflow_(input__clipped_dataset).ga \n  -  extra-files/metavisitor/Galaxy-Workflow-Metavisitor_Test_case_1-1_Guided.ga \n  -  extra-files/metavisitor/Galaxy-Workflow-Metavisitor_Test_case_1-2_Guided.ga \n  -  extra-files/metavisitor/Galaxy-Workflow-Metavisitor_Test_case_1-3_Guided.ga \n  -  extra-files/metavisitor/Galaxy-Workflow-Meta-visitor__test_case_Nora_virus,_REMAPPING.ga", 
            "title": "Installing workflows"
        }, 
        {
            "location": "/installing tools and workflows/#running-the-playbook", 
            "text": "As per usual, run the playbook with an inventory file that maps your target machine to the metavisitor group.\nIf the target is localhost, your inventory file should look ike this:  [metavisitor]\nlocalhost  then run the playbook like so:  ansible-playbook --inventory-file= your_inventory_file  galaxy.yml", 
            "title": "Running the playbook"
        }, 
        {
            "location": "/examples/docker/", 
            "text": "Building and deploying galaxy-kickstart in docker\n\n\n\n\nRequirements\n\n\nYou need to have docker installed and configured for your user.\n\n\nThe repository comes with various Dockerfiles that can be used to configure a deployment using Docker,\nor you can start with a pre-built docker image.\n\n\nRunning images from the dockerhub\n\n\nYou can obtain a pre-built docker image from the dockerhub:\n\n\ndocker pull artbio/galaxy-kickstart-base\n\n\n\n\nStart the image and serve it on port 8080 of your local machine in the standard docker way:\n\n\nCID=`docker run -d -p 8080:80 artbio/galaxy-kickstart-base`\n\n\n\n\n-p 8080:80\n will forward requests to nginx inside the container running on port 80.\n\n\nRuntime changes to pre-built docker images\n\n\nIf you wish to reach the container on a subdirectory, add \n-e NGINX_GALAXY_LOCATION=\"/my-subdirectory\"\n to the docker call \nand galaxy will be served at \nhttp://127.0.0.1:8080/my-subdirectory\n.\n\n\nWe recommend changing the default admin user as well, so the command becomes:\n\n\nCID=`docker run -d -e NGINX_GALAXY_LOCATION=\n/my-subdirectory\n -e GALAXY_CONFIG_ADMIN_USERS=admin@artbio.fr -p 8080:80 artbio/galaxy-kickstart-base`\n\n\n\n\nCommit changed containers to new images\n\n\nAs with standard docker containers, you can change, tag and commit running containers when you have configured them to your liking:\nCommit the changes to my-new-image\n\n\ndocker commit $CID my-new-image\n\n\n\n\n\nStop and remove the original container:\n\n\ndocker stop $CID \n docker rm $CID\n\n\n\n\nStart the new container:\n\n\nCID=`docker run -d -e NGINX_GALAXY_LOCATION=\n/my-subdirectory\n -e GALAXY_CONFIG_ADMIN_USERS=admin@artbio.fr -p 8080:80 my-new-image`\n\n\n\n\nPersisting to disk\n\n\nAll changes made to the container are by default ephemeral; if you remove the container, the changes are gone.\nTo persist data (this includes the postgresql database, galaxy's config files and your user data), mount a Volume into\nthe containers /export folder.\nDue to the persistance mechanism (we use bind-mounts inside the container), you need to privilege the container.\nAssuming you would like to mount your local \n/data\n folder, run\n\n\nCID=`docker run -d --privileged -v /data:/export -p 8080:80 my-new-image`\n\n\n\n\nThis will run through the persistence tags of the galaxy.yml and export the required files to /export (now on your machine's /data).\nFrom the new location the files are being bind-mounted back into their original location.", 
            "title": "Docker"
        }, 
        {
            "location": "/examples/docker/#building-and-deploying-galaxy-kickstart-in-docker", 
            "text": "", 
            "title": "Building and deploying galaxy-kickstart in docker"
        }, 
        {
            "location": "/examples/docker/#requirements", 
            "text": "You need to have docker installed and configured for your user.  The repository comes with various Dockerfiles that can be used to configure a deployment using Docker,\nor you can start with a pre-built docker image.", 
            "title": "Requirements"
        }, 
        {
            "location": "/examples/docker/#running-images-from-the-dockerhub", 
            "text": "You can obtain a pre-built docker image from the dockerhub:  docker pull artbio/galaxy-kickstart-base  Start the image and serve it on port 8080 of your local machine in the standard docker way:  CID=`docker run -d -p 8080:80 artbio/galaxy-kickstart-base`  -p 8080:80  will forward requests to nginx inside the container running on port 80.", 
            "title": "Running images from the dockerhub"
        }, 
        {
            "location": "/examples/docker/#runtime-changes-to-pre-built-docker-images", 
            "text": "If you wish to reach the container on a subdirectory, add  -e NGINX_GALAXY_LOCATION=\"/my-subdirectory\"  to the docker call \nand galaxy will be served at  http://127.0.0.1:8080/my-subdirectory .  We recommend changing the default admin user as well, so the command becomes:  CID=`docker run -d -e NGINX_GALAXY_LOCATION= /my-subdirectory  -e GALAXY_CONFIG_ADMIN_USERS=admin@artbio.fr -p 8080:80 artbio/galaxy-kickstart-base`", 
            "title": "Runtime changes to pre-built docker images"
        }, 
        {
            "location": "/examples/docker/#commit-changed-containers-to-new-images", 
            "text": "As with standard docker containers, you can change, tag and commit running containers when you have configured them to your liking:\nCommit the changes to my-new-image  docker commit $CID my-new-image  Stop and remove the original container:  docker stop $CID   docker rm $CID  Start the new container:  CID=`docker run -d -e NGINX_GALAXY_LOCATION= /my-subdirectory  -e GALAXY_CONFIG_ADMIN_USERS=admin@artbio.fr -p 8080:80 my-new-image`", 
            "title": "Commit changed containers to new images"
        }, 
        {
            "location": "/examples/docker/#persisting-to-disk", 
            "text": "All changes made to the container are by default ephemeral; if you remove the container, the changes are gone.\nTo persist data (this includes the postgresql database, galaxy's config files and your user data), mount a Volume into\nthe containers /export folder.\nDue to the persistance mechanism (we use bind-mounts inside the container), you need to privilege the container.\nAssuming you would like to mount your local  /data  folder, run  CID=`docker run -d --privileged -v /data:/export -p 8080:80 my-new-image`  This will run through the persistence tags of the galaxy.yml and export the required files to /export (now on your machine's /data).\nFrom the new location the files are being bind-mounted back into their original location.", 
            "title": "Persisting to disk"
        }, 
        {
            "location": "/examples/vagrant/", 
            "text": "Deploying galaxy-kickstart on local virtual machine (VM) using vagrant.\n\n\n\n\nGalaxyKickStart is designed to be flexible and powerful, but for demonstration purposes we start a simple vagrant box\nthat runs this playbook. Following these instructions will not change the host system.\nAlternatively, see \nexamples/docker\n for running the playbook in docker,\nor \ngetting started\n for running the playbook on local or remote machines.\n\n\nRequirements\n\n\nTo follow the examples \nansible\n, \nvagrant\n\nand \ngit\n need to be installed.\n\n\nRunning the playbook on a Virtual Machine\n\n\nThe Vagrantfile describes a Virtual Machine (VM) that is based on Ubuntu 14.04 (codename trusty).\n\n\nVAGRANTFILE_API_VERSION = \n2\n\n   Vagrant.configure(VAGRANTFILE_API_VERSION) do |config|\n      config.vm.box = \nubuntu/trusty64\n\n      config.vm.network \nforwarded_port\n, guest: 80, host: 8080\n      config.vm.network \nforwarded_port\n, guest: 21, host: 2121\n\n      config.vm.provider \nvirtualbox\n do |v|\n         v.memory = 4096\n      end\n\n      config.vm.provision \nansible\n do |ansible|\n         ansible.extra_vars = {\n            ntp_server: \npool.ntp.org\n,\n            ansible_ssh_user: 'vagrant'\n         }\n         ansible.verbose = 'vvvv'\n         ansible.playbook = \ngalaxy.yml\n\n      end\n   end\n\n\n\n\nBy default, port 8080 will be forwarded to port 80, and port 2121 will be forwarded to port 21 (for FTP),\nand 4096 MB of memory will be attributed to the VM.\nEnter the playbook directory \ncd GalaxyKickStart\n and type \nvagrant up\n to download a VM image and run the \ngalaxy.yml\n playbook.\n\n\nThis will take a while. Once finished, you should find a running Galaxy Instance on http://localhost:8080 .\nIf you would like to see the internals of the VM, you can log into the machine by typing \nvagrant ssh\n.\n\n\nvagrant up\n makes use of the ansible provisioner and is equivalent of starting a vagrant machine without the ansible provisioner\nand running ansible through an ssh connection to the vagrant machine (which listens by default on port 2222)\nThe hosts inventory file contains an example for directly pointing ansible to the vagrant machine.\nUncomment the vagrant specific lines and comment or remove the remaining lines:\n\n\n#[artimed]\n#localhost ansible_ssh_user=\nroot\n ansible_ssh_private_key_file=\n~/.ssh/id_rsa\n\n#[travis_bioblend]\n#localhost ansible_connection=local\n# Uncomment the 2 lines below to point ansible to a local vagrant machine.\n[all]\nlocalhost ansible_user=\nvagrant\n ansible_port=2222 ansible_private_key_file=.vagrant/machines/default/virtualbox/private_key\n#[aws]\n# Put you aws IP and key here to make FTP work in the default VPC.\n# If you want further group-specific variables, put the host in these groups as well [e.g artimed].\n\n\n\n\nTo run the playbook again, type\n\n\nansible-playbook --inventory-file=\nyour_inventory\n galaxy.yml\n\n\n\n\nCleaning up\n\n\nThe VM image and various config files have been written to the \n.vagrant\n folder. Type \nvagrant halt\n to stop the running instance\nand \nvagrant destroy\n to remove the VM, and then delete the \n.vagrant\n folder.", 
            "title": "Vagrant"
        }, 
        {
            "location": "/examples/vagrant/#deploying-galaxy-kickstart-on-local-virtual-machine-vm-using-vagrant", 
            "text": "GalaxyKickStart is designed to be flexible and powerful, but for demonstration purposes we start a simple vagrant box\nthat runs this playbook. Following these instructions will not change the host system.\nAlternatively, see  examples/docker  for running the playbook in docker,\nor  getting started  for running the playbook on local or remote machines.", 
            "title": "Deploying galaxy-kickstart on local virtual machine (VM) using vagrant."
        }, 
        {
            "location": "/examples/vagrant/#requirements", 
            "text": "To follow the examples  ansible ,  vagrant \nand  git  need to be installed.", 
            "title": "Requirements"
        }, 
        {
            "location": "/examples/vagrant/#running-the-playbook-on-a-virtual-machine", 
            "text": "The Vagrantfile describes a Virtual Machine (VM) that is based on Ubuntu 14.04 (codename trusty).  VAGRANTFILE_API_VERSION =  2 \n   Vagrant.configure(VAGRANTFILE_API_VERSION) do |config|\n      config.vm.box =  ubuntu/trusty64 \n      config.vm.network  forwarded_port , guest: 80, host: 8080\n      config.vm.network  forwarded_port , guest: 21, host: 2121\n\n      config.vm.provider  virtualbox  do |v|\n         v.memory = 4096\n      end\n\n      config.vm.provision  ansible  do |ansible|\n         ansible.extra_vars = {\n            ntp_server:  pool.ntp.org ,\n            ansible_ssh_user: 'vagrant'\n         }\n         ansible.verbose = 'vvvv'\n         ansible.playbook =  galaxy.yml \n      end\n   end  By default, port 8080 will be forwarded to port 80, and port 2121 will be forwarded to port 21 (for FTP),\nand 4096 MB of memory will be attributed to the VM.\nEnter the playbook directory  cd GalaxyKickStart  and type  vagrant up  to download a VM image and run the  galaxy.yml  playbook.  This will take a while. Once finished, you should find a running Galaxy Instance on http://localhost:8080 .\nIf you would like to see the internals of the VM, you can log into the machine by typing  vagrant ssh .  vagrant up  makes use of the ansible provisioner and is equivalent of starting a vagrant machine without the ansible provisioner\nand running ansible through an ssh connection to the vagrant machine (which listens by default on port 2222)\nThe hosts inventory file contains an example for directly pointing ansible to the vagrant machine.\nUncomment the vagrant specific lines and comment or remove the remaining lines:  #[artimed]\n#localhost ansible_ssh_user= root  ansible_ssh_private_key_file= ~/.ssh/id_rsa \n#[travis_bioblend]\n#localhost ansible_connection=local\n# Uncomment the 2 lines below to point ansible to a local vagrant machine.\n[all]\nlocalhost ansible_user= vagrant  ansible_port=2222 ansible_private_key_file=.vagrant/machines/default/virtualbox/private_key\n#[aws]\n# Put you aws IP and key here to make FTP work in the default VPC.\n# If you want further group-specific variables, put the host in these groups as well [e.g artimed].  To run the playbook again, type  ansible-playbook --inventory-file= your_inventory  galaxy.yml", 
            "title": "Running the playbook on a Virtual Machine"
        }, 
        {
            "location": "/examples/vagrant/#cleaning-up", 
            "text": "The VM image and various config files have been written to the  .vagrant  folder. Type  vagrant halt  to stop the running instance\nand  vagrant destroy  to remove the VM, and then delete the  .vagrant  folder.", 
            "title": "Cleaning up"
        }, 
        {
            "location": "/faq/", 
            "text": "Why does the playbook fail?\n\n\nMake sure that you are on ansible version \n=2.1.\nYou can check your ansible version by typing:\n\n\nansible --version\n\n\n\n\nWhat is the username and password of the galaxy admin account ?\n\n\nUsername and password of the galaxy account are controlled by the variables \ngalaxy_admin\n and \ngalaxy_admin_pw\n and\ndefault to \nadmin@galaxy.org\n and \nadmin\n (Defaults are defined in group_vars/all). This should be changed in the group or host variables for the host you are working on.\nIf you have a host in the \nmygroup\n group, you can edit group_vars/my_group and set\n\n\ngalaxy_admin: new_admin@email.com\ngalaxy_admin_pw: new_password\n\n\n\n\nAs with each change, run the playbook again.", 
            "title": "Frequently asked questions"
        }, 
        {
            "location": "/faq/#why-does-the-playbook-fail", 
            "text": "Make sure that you are on ansible version  =2.1.\nYou can check your ansible version by typing:  ansible --version", 
            "title": "Why does the playbook fail?"
        }, 
        {
            "location": "/faq/#what-is-the-username-and-password-of-the-galaxy-admin-account", 
            "text": "Username and password of the galaxy account are controlled by the variables  galaxy_admin  and  galaxy_admin_pw  and\ndefault to  admin@galaxy.org  and  admin  (Defaults are defined in group_vars/all). This should be changed in the group or host variables for the host you are working on.\nIf you have a host in the  mygroup  group, you can edit group_vars/my_group and set  galaxy_admin: new_admin@email.com\ngalaxy_admin_pw: new_password  As with each change, run the playbook again.", 
            "title": "What is the username and password of the galaxy admin account ?"
        }, 
        {
            "location": "/available_roles/", 
            "text": "ansible-postgresql-objects role\n\n\nensure_postgresql_up role\n\n\ngalaxy-extras role\n\n\ngalaxy-tools role\n\n\ngalaxy-os role\n\n\ngalaxy role\n\n\nansible-trackster role\n\n\nminiconda role", 
            "title": "Available roles"
        }, 
        {
            "location": "/available_variables/", 
            "text": "", 
            "title": "Available variables"
        }, 
        {
            "location": "/metavisitor/", 
            "text": "Metavisitor\n is a set of Galaxy tools and workflows to detect and reconstruct viral genomes from complex deep sequence datasets.\n\n\nDocumentation on Metavisitor installation using GalaxyKickStart is available in the \nMetavisitor manual", 
            "title": "Metavisitor"
        }, 
        {
            "location": "/GKS2slurm/", 
            "text": "What is GKS2slurm ?\n\n\nGKS2slurm is a playbook that is played to install a multinode slurm cluster over a GalaxyKickStart single-node installation.\nThe playbook GKS2slurm \ngalaxyToSlurmCluster.yml\n was tested with multiple virtual machines (VMs) in \nStratuslab\n, \nGoogle Cloud Engine (GCE)\n and \nAmazon Web Services (AWS)\n clouds.\n\n\nInstallation of a Galaxy slurm cluster with GKS2slurm\n\n\nStep 1: Install a Galaxy server with GalaxyKickStart\n\n\n\n\n\n\nReport to the \nGetting Started\n section of this manual for the basics of GalaxyKickStart installation\n\n\n\n\n\n\ninstall any GalaxyKickStart \"flavor\" by configuring the inventory file (in inventory_files folder) and the group_vars file (in the group_vars folder) of your choice.\nFlavors currently available are \nkickstart\n, \nartimed\n and \nmetavisitor\n but other will come soon. Alternatively, you can build you own flavor by customizing a group_vars, extrafiles file and inventory file, which will install your Galaxy tools and workflows.\n\n\nin Step 1, the most important thing to keep track with is to configure your target machine with an extra volume\n\n\nIndeed GKS2slurm has be designed so that the Galaxy slurm cluster can accumulate large amount of data in the long term, which can be more easily shared with the cluster nodes and more importantly backed up.\n\n\nThus in addition of all the adaptations you will do for your own purpose (tools, workflows, etc), edit the \ngroup_vars/all\n file and adapt the \ngalaxy_persistent_directory\n variable to your extra volume which should be already formatted and mounted:\n\n\nChange\n\n\n\n\n\n\n#persistent data\ngalaxy_persistent_directory: /export # for IFB it's /root/mydisk, by default, /export\n\n\n\n\nTo\n\n\n#persistent data\ngalaxy_persistent_directory: /pathto/mounted/extravolume\n\n\n\n\n\n\nHaving configured your GalaxyKickStart installation, import the extra roles (if not already done)\n\n\n\n\nansible-galaxy install -r requirements_roles.yml -p roles\n\n\n\n\nand run the galaxy.yml playbook:\n\n\nansible-playbook --inventory-file inventory_files/\nyour_inventory_file\n galaxy.yml\n\n\n\n\nStep 2: Check the single node Galaxy installation\n\n\nIf the playbook was run successfully, connect to your Galaxy instance through http and check that you can login (admin@galaxy.org:admin), and that tools and workflows are correctly installed.\n\n\nStep 3: Moving your single node configuration to a multinode slurm configuration\n\n\n\n\nStart as many compute nodes you want for the slurm cluster and gather information from each node:\n\n\nIP address (all slurm nodes should must be accessible in the same network, ie nodes can be ping-ed from any nodes)\n\n\nhostname\n\n\nnumber of CPUs\n\n\nmemory (in MB)\n\n\n\n\n\n\n\n\nStep 3-1\n\n\nAdapt the inventory file \nslurm-kickstart\n in the inventory_files folder.\n\n\n[slurm_master]\n# adapt the following line to IP address and ssh user of the slurm master node\n192.54.201.102 ansible_ssh_user=root ansible_ssh_private_key_file=\n~/.ssh/mysshkey\n\n\n[slurm_slave]\n# adapt the following lnes to IP addresses and ssu users of the slum slave nodes\n192.54.201.98 ansible_ssh_user=root ansible_ssh_private_key_file=\n~/.ssh/mysshkey\n\n192.54.201.99 ansible_ssh_user=root ansible_ssh_private_key_file=\n~/.ssh/mysshkey\n\n192.54.201.101 ansible_ssh_user=root ansible_ssh_private_key_file=\n~/.ssh/mysshkey\n\n\n\n\n\nStep 3-2\n\n\nAdapt the group_vars file \nslurm_master\n in the \ngroup_vars\n folder.\nThis is done using the information gathered in step 3\n\n\n# nfs sharing\ncluster_ip_range: \n0.0.0.0/24\n # replace by your ip network range\n\n# slave node specifications, adapt to your set of slave nodes\nslave_node_dict:\n  - {hostname: \nslave-1\n, CPUs: \n2\n, RealMemory: \n7985\n}\n  - {hostname: \nslave-2\n, CPUs: \n2\n, RealMemory: \n7985\n}\n  - {hostname: \nslave-3\n, CPUs: \n2\n, RealMemory: \n7985\n}\n\n\n\n\nStep 3-3\n\n\nAdapt the group_vars file \nslurm_slave\n in the \ngroup_vars\n folder\n\n\n# adapt the following variable to the master slurm node IP address\nmaster_slurm_node_ip: \n192.54.201.102\n\n\n\n\n\nStep 3-4\n\n\nRun the playbook \ngalaxyToSlurmCluster.yml\n playbook.\nfrom the GalaxyKickStart directory:\n\n\nansible-playbook -i inventory_files/slurm-kickstart galaxyToSlurmCluster.yml\n\n\n\n\nNote that if you configure multiple slave nodes without prior ssh key authentification, you can run the same command with the variable ANSIBLE_HOST_KEY_CHECKING put to False:\n\n\nANSIBLE_HOST_KEY_CHECKING=False ansible-playbook -i inventory_files/slurm-kickstart galaxyToSlurmCluster.yml\n\n\n\n\nChecking slurm installation\n\n\nConnect to your master node as root and type \nsinfo\n\nRefer to slurm documentation for more investigation/control", 
            "title": "GKS2slurm"
        }, 
        {
            "location": "/GKS2slurm/#what-is-gks2slurm", 
            "text": "GKS2slurm is a playbook that is played to install a multinode slurm cluster over a GalaxyKickStart single-node installation.\nThe playbook GKS2slurm  galaxyToSlurmCluster.yml  was tested with multiple virtual machines (VMs) in  Stratuslab ,  Google Cloud Engine (GCE)  and  Amazon Web Services (AWS)  clouds.", 
            "title": "What is GKS2slurm ?"
        }, 
        {
            "location": "/GKS2slurm/#installation-of-a-galaxy-slurm-cluster-with-gks2slurm", 
            "text": "", 
            "title": "Installation of a Galaxy slurm cluster with GKS2slurm"
        }, 
        {
            "location": "/GKS2slurm/#step-1-install-a-galaxy-server-with-galaxykickstart", 
            "text": "Report to the  Getting Started  section of this manual for the basics of GalaxyKickStart installation    install any GalaxyKickStart \"flavor\" by configuring the inventory file (in inventory_files folder) and the group_vars file (in the group_vars folder) of your choice.\nFlavors currently available are  kickstart ,  artimed  and  metavisitor  but other will come soon. Alternatively, you can build you own flavor by customizing a group_vars, extrafiles file and inventory file, which will install your Galaxy tools and workflows.  in Step 1, the most important thing to keep track with is to configure your target machine with an extra volume  Indeed GKS2slurm has be designed so that the Galaxy slurm cluster can accumulate large amount of data in the long term, which can be more easily shared with the cluster nodes and more importantly backed up.  Thus in addition of all the adaptations you will do for your own purpose (tools, workflows, etc), edit the  group_vars/all  file and adapt the  galaxy_persistent_directory  variable to your extra volume which should be already formatted and mounted:  Change    #persistent data\ngalaxy_persistent_directory: /export # for IFB it's /root/mydisk, by default, /export  To  #persistent data\ngalaxy_persistent_directory: /pathto/mounted/extravolume   Having configured your GalaxyKickStart installation, import the extra roles (if not already done)   ansible-galaxy install -r requirements_roles.yml -p roles  and run the galaxy.yml playbook:  ansible-playbook --inventory-file inventory_files/ your_inventory_file  galaxy.yml", 
            "title": "Step 1: Install a Galaxy server with GalaxyKickStart"
        }, 
        {
            "location": "/GKS2slurm/#step-2-check-the-single-node-galaxy-installation", 
            "text": "If the playbook was run successfully, connect to your Galaxy instance through http and check that you can login (admin@galaxy.org:admin), and that tools and workflows are correctly installed.", 
            "title": "Step 2: Check the single node Galaxy installation"
        }, 
        {
            "location": "/GKS2slurm/#step-3-moving-your-single-node-configuration-to-a-multinode-slurm-configuration", 
            "text": "Start as many compute nodes you want for the slurm cluster and gather information from each node:  IP address (all slurm nodes should must be accessible in the same network, ie nodes can be ping-ed from any nodes)  hostname  number of CPUs  memory (in MB)", 
            "title": "Step 3: Moving your single node configuration to a multinode slurm configuration"
        }, 
        {
            "location": "/GKS2slurm/#step-3-1", 
            "text": "Adapt the inventory file  slurm-kickstart  in the inventory_files folder.  [slurm_master]\n# adapt the following line to IP address and ssh user of the slurm master node\n192.54.201.102 ansible_ssh_user=root ansible_ssh_private_key_file= ~/.ssh/mysshkey \n\n[slurm_slave]\n# adapt the following lnes to IP addresses and ssu users of the slum slave nodes\n192.54.201.98 ansible_ssh_user=root ansible_ssh_private_key_file= ~/.ssh/mysshkey \n192.54.201.99 ansible_ssh_user=root ansible_ssh_private_key_file= ~/.ssh/mysshkey \n192.54.201.101 ansible_ssh_user=root ansible_ssh_private_key_file= ~/.ssh/mysshkey", 
            "title": "Step 3-1"
        }, 
        {
            "location": "/GKS2slurm/#step-3-2", 
            "text": "Adapt the group_vars file  slurm_master  in the  group_vars  folder.\nThis is done using the information gathered in step 3  # nfs sharing\ncluster_ip_range:  0.0.0.0/24  # replace by your ip network range\n\n# slave node specifications, adapt to your set of slave nodes\nslave_node_dict:\n  - {hostname:  slave-1 , CPUs:  2 , RealMemory:  7985 }\n  - {hostname:  slave-2 , CPUs:  2 , RealMemory:  7985 }\n  - {hostname:  slave-3 , CPUs:  2 , RealMemory:  7985 }", 
            "title": "Step 3-2"
        }, 
        {
            "location": "/GKS2slurm/#step-3-3", 
            "text": "Adapt the group_vars file  slurm_slave  in the  group_vars  folder  # adapt the following variable to the master slurm node IP address\nmaster_slurm_node_ip:  192.54.201.102", 
            "title": "Step 3-3"
        }, 
        {
            "location": "/GKS2slurm/#step-3-4", 
            "text": "Run the playbook  galaxyToSlurmCluster.yml  playbook.\nfrom the GalaxyKickStart directory:  ansible-playbook -i inventory_files/slurm-kickstart galaxyToSlurmCluster.yml  Note that if you configure multiple slave nodes without prior ssh key authentification, you can run the same command with the variable ANSIBLE_HOST_KEY_CHECKING put to False:  ANSIBLE_HOST_KEY_CHECKING=False ansible-playbook -i inventory_files/slurm-kickstart galaxyToSlurmCluster.yml", 
            "title": "Step 3-4"
        }, 
        {
            "location": "/GKS2slurm/#checking-slurm-installation", 
            "text": "Connect to your master node as root and type  sinfo \nRefer to slurm documentation for more investigation/control", 
            "title": "Checking slurm installation"
        }, 
        {
            "location": "/GKSfromWorflows/", 
            "text": "GKSfromWorflows\n\n\nGKSfromWorflows uses the python script \ngenerate_tool_list_from_ga_workflow_files.py\n to quickly generate a GalaxyKickStart use case from one or several workflow files (\n.ga\n, note that these files must have been generated with galaxy \n= release_16.04)\n\n\nFrom GalaxyKickStart/scripts, run\n\n\npython generate_tool_list_from_ga_workflow_files.py --help\n\n\n\n\nThen\n\n\npython generate_tool_list_from_ga_workflow_files.py -w \nworkflow1.ga\n \nworkflow2.ga\n ... -l \nPanel_label\n\n\n\n\n\nThis creates:\n\n\n\n\nAn inventory file \nGKSfromWorkflow\n in the \ninventory_files\nfolder\n\n\nA group_vars file \nGKSfromWorkflow\n in the \ngroup_vars\nfolder\n\n\nA folder \nGKSfromWorkflow\nin the folder \nextra-files\n which will contain a copy of the workflow1.ga, workflow2.ga, ... files, plus a \nGKSfromWorkflow_tool_list.yml\n file that contains a yml description of all tools used in the workflows.\n\n\n\n\nNote that running \ngenerate_tool_list_from_ga_workflow_files.py\n overwrites these folders and files if they exist from a previous script run.\n\n\nAdapt the created inventory file\n\n\nBefore running ansible-playbook, you have just to adapt the \nGKSfromWorkflow\n inventory_file \ninventory_files/GKSfromWorkflow\n to your own network settings (the file is preconfigured for running locally ansible-playbook on your target machine).\n\n\nAs usual, you may also tune the \ngroup_vars/all\n file.\n\n\nRun the playbook\n\n\ncd GalaxyKickStart\nansible-galaxy install -r requirements_roles.yml -p roles\nansible-playbook -i inventory_files/GKSfromWorkflow galaxy.yml\n\n\n\n\nCheck your running Galaxy instance after completion of the playbook. It contains the preinstalled tools as well as the workflow1, workflow2, etc....\n\n\nIssue currently addressed\n\n\nIn some occasions, running the playbook with GKSfromWorkflow may end up with tool versions that are not the ones specified in the generated \nGKSfromWorkflow_tool_list.yml\n tool list.\n\n\nThis may cause the workflow(s) to crash. Should this happens, carefully check the installed tool version and install the exact version required. It is possible that some conda packages are used instead of the toolshed dependencies specified by some outdated tool versions.", 
            "title": "GKSfromWorflows"
        }, 
        {
            "location": "/GKSfromWorflows/#gksfromworflows", 
            "text": "GKSfromWorflows uses the python script  generate_tool_list_from_ga_workflow_files.py  to quickly generate a GalaxyKickStart use case from one or several workflow files ( .ga , note that these files must have been generated with galaxy  = release_16.04)", 
            "title": "GKSfromWorflows"
        }, 
        {
            "location": "/GKSfromWorflows/#from-galaxykickstartscripts-run", 
            "text": "python generate_tool_list_from_ga_workflow_files.py --help  Then  python generate_tool_list_from_ga_workflow_files.py -w  workflow1.ga   workflow2.ga  ... -l  Panel_label   This creates:   An inventory file  GKSfromWorkflow  in the  inventory_files folder  A group_vars file  GKSfromWorkflow  in the  group_vars folder  A folder  GKSfromWorkflow in the folder  extra-files  which will contain a copy of the workflow1.ga, workflow2.ga, ... files, plus a  GKSfromWorkflow_tool_list.yml  file that contains a yml description of all tools used in the workflows.   Note that running  generate_tool_list_from_ga_workflow_files.py  overwrites these folders and files if they exist from a previous script run.", 
            "title": "From GalaxyKickStart/scripts, run"
        }, 
        {
            "location": "/GKSfromWorflows/#adapt-the-created-inventory-file", 
            "text": "Before running ansible-playbook, you have just to adapt the  GKSfromWorkflow  inventory_file  inventory_files/GKSfromWorkflow  to your own network settings (the file is preconfigured for running locally ansible-playbook on your target machine).  As usual, you may also tune the  group_vars/all  file.", 
            "title": "Adapt the created inventory file"
        }, 
        {
            "location": "/GKSfromWorflows/#run-the-playbook", 
            "text": "cd GalaxyKickStart\nansible-galaxy install -r requirements_roles.yml -p roles\nansible-playbook -i inventory_files/GKSfromWorkflow galaxy.yml  Check your running Galaxy instance after completion of the playbook. It contains the preinstalled tools as well as the workflow1, workflow2, etc....", 
            "title": "Run the playbook"
        }, 
        {
            "location": "/GKSfromWorflows/#issue-currently-addressed", 
            "text": "In some occasions, running the playbook with GKSfromWorkflow may end up with tool versions that are not the ones specified in the generated  GKSfromWorkflow_tool_list.yml  tool list.  This may cause the workflow(s) to crash. Should this happens, carefully check the installed tool version and install the exact version required. It is possible that some conda packages are used instead of the toolshed dependencies specified by some outdated tool versions.", 
            "title": "Issue currently addressed"
        }
    ]
}